{"cells":[{"cell_type":"markdown","metadata":{},"source":["<a name=\"ilp\"></a>\n","# Import necessary libraries and packages "]},{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-02-18T11:05:50.432505Z","iopub.status.busy":"2023-02-18T11:05:50.431346Z","iopub.status.idle":"2023-02-18T11:05:57.805680Z","shell.execute_reply":"2023-02-18T11:05:57.804462Z","shell.execute_reply.started":"2023-02-18T11:05:50.432323Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","import os\n","import numpy as np\n","\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.optimizers import Adam"]},{"cell_type":"markdown","metadata":{},"source":["<a name=\"di\"></a>\n","# Dataset information\n","\n","**Import Medium-articles-dataset:**\n","\n","This dataset contains information about randomly chosen medium articles published in 2019 from these 7 publications:\n","\n","- Towards Data Science\n","- UX Collective\n","- The Startup\n","- The Writing Cooperative\n","- Data Driven Investor\n","- Better Humans\n","- Better Marketing\n"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-02-18T11:07:43.575425Z","iopub.status.busy":"2023-02-18T11:07:43.575018Z","iopub.status.idle":"2023-02-18T11:07:43.690501Z","shell.execute_reply":"2023-02-18T11:07:43.689232Z","shell.execute_reply.started":"2023-02-18T11:07:43.575378Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>url</th>\n","      <th>title</th>\n","      <th>subtitle</th>\n","      <th>image</th>\n","      <th>claps</th>\n","      <th>responses</th>\n","      <th>reading_time</th>\n","      <th>publication</th>\n","      <th>date</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>https://towardsdatascience.com/a-beginners-gui...</td>\n","      <td>A Beginner’s Guide to Word Embedding with Gens...</td>\n","      <td>NaN</td>\n","      <td>1.png</td>\n","      <td>850</td>\n","      <td>8</td>\n","      <td>8</td>\n","      <td>Towards Data Science</td>\n","      <td>2019-05-30</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>https://towardsdatascience.com/hands-on-graph-...</td>\n","      <td>Hands-on Graph Neural Networks with PyTorch &amp; ...</td>\n","      <td>NaN</td>\n","      <td>2.png</td>\n","      <td>1100</td>\n","      <td>11</td>\n","      <td>9</td>\n","      <td>Towards Data Science</td>\n","      <td>2019-05-30</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>https://towardsdatascience.com/how-to-use-ggpl...</td>\n","      <td>How to Use ggplot2 in Python</td>\n","      <td>A Grammar of Graphics for Python</td>\n","      <td>3.png</td>\n","      <td>767</td>\n","      <td>1</td>\n","      <td>5</td>\n","      <td>Towards Data Science</td>\n","      <td>2019-05-30</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>https://towardsdatascience.com/databricks-how-...</td>\n","      <td>Databricks: How to Save Files in CSV on Your L...</td>\n","      <td>When I work on Python projects dealing…</td>\n","      <td>4.jpeg</td>\n","      <td>354</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>Towards Data Science</td>\n","      <td>2019-05-30</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>https://towardsdatascience.com/a-step-by-step-...</td>\n","      <td>A Step-by-Step Implementation of Gradient Desc...</td>\n","      <td>One example of building neural…</td>\n","      <td>5.jpeg</td>\n","      <td>211</td>\n","      <td>3</td>\n","      <td>4</td>\n","      <td>Towards Data Science</td>\n","      <td>2019-05-30</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   id                                                url  \\\n","0   1  https://towardsdatascience.com/a-beginners-gui...   \n","1   2  https://towardsdatascience.com/hands-on-graph-...   \n","2   3  https://towardsdatascience.com/how-to-use-ggpl...   \n","3   4  https://towardsdatascience.com/databricks-how-...   \n","4   5  https://towardsdatascience.com/a-step-by-step-...   \n","\n","                                               title  \\\n","0  A Beginner’s Guide to Word Embedding with Gens...   \n","1  Hands-on Graph Neural Networks with PyTorch & ...   \n","2                       How to Use ggplot2 in Python   \n","3  Databricks: How to Save Files in CSV on Your L...   \n","4  A Step-by-Step Implementation of Gradient Desc...   \n","\n","                                  subtitle   image  claps responses  \\\n","0                                      NaN   1.png    850         8   \n","1                                      NaN   2.png   1100        11   \n","2         A Grammar of Graphics for Python   3.png    767         1   \n","3  When I work on Python projects dealing…  4.jpeg    354         0   \n","4          One example of building neural…  5.jpeg    211         3   \n","\n","   reading_time           publication        date  \n","0             8  Towards Data Science  2019-05-30  \n","1             9  Towards Data Science  2019-05-30  \n","2             5  Towards Data Science  2019-05-30  \n","3             4  Towards Data Science  2019-05-30  \n","4             4  Towards Data Science  2019-05-30  "]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["medium_data = pd.read_csv('../input/medium-articles-dataset/medium_data.csv')\n","medium_data.head()"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"markdown","metadata":{},"source":["Here, we have a **10 different fields and 6508 records** but we will only use **title field** for predicting next word. "]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-02-18T11:07:45.074646Z","iopub.status.busy":"2023-02-18T11:07:45.074135Z","iopub.status.idle":"2023-02-18T11:07:45.082640Z","shell.execute_reply":"2023-02-18T11:07:45.081103Z","shell.execute_reply.started":"2023-02-18T11:07:45.074604Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of records:  6508\n","Number of fields:  10\n"]}],"source":["print(\"Number of records: \", medium_data.shape[0])\n","print(\"Number of fields: \", medium_data.shape[1])"]},{"cell_type":"markdown","metadata":{},"source":["<a name=\"preprocess\"></a>\n","# Display titles of various articles  and preprocess them"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-02-18T11:07:45.365516Z","iopub.status.busy":"2023-02-18T11:07:45.364932Z","iopub.status.idle":"2023-02-18T11:07:45.373964Z","shell.execute_reply":"2023-02-18T11:07:45.372498Z","shell.execute_reply.started":"2023-02-18T11:07:45.365477Z"},"trusted":true},"outputs":[{"data":{"text/plain":["'A Beginner’s Guide to Word Embedding with Gensim Word2Vec\\xa0Model'"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["medium_data['title'][0]"]},{"cell_type":"markdown","metadata":{},"source":["<a name=\"remv\"></a>\n","#### Removing unwanted characters and words in titles\n","\n","Looking at titles, we can see there are some of unwanted characters and words in it which can not be useful for us to predict infact it might decrease our model accuracy so we have to remove it."]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-02-18T11:07:45.764047Z","iopub.status.busy":"2023-02-18T11:07:45.763600Z","iopub.status.idle":"2023-02-18T11:07:45.778464Z","shell.execute_reply":"2023-02-18T11:07:45.777084Z","shell.execute_reply.started":"2023-02-18T11:07:45.764003Z"},"trusted":true},"outputs":[],"source":["medium_data['title'] = medium_data['title'].apply(lambda x: x.replace(u'\\xa0',u' '))\n","medium_data['title'] = medium_data['title'].apply(lambda x: x.replace('\\u200a',' '))"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-02-18T11:07:45.966760Z","iopub.status.busy":"2023-02-18T11:07:45.966352Z","iopub.status.idle":"2023-02-18T11:07:45.976980Z","shell.execute_reply":"2023-02-18T11:07:45.975795Z","shell.execute_reply.started":"2023-02-18T11:07:45.966723Z"},"trusted":true},"outputs":[{"data":{"text/plain":["0       A Beginner’s Guide to Word Embedding with Gens...\n","1       Hands-on Graph Neural Networks with PyTorch & ...\n","2                            How to Use ggplot2 in Python\n","3       Databricks: How to Save Files in CSV on Your L...\n","4       A Step-by-Step Implementation of Gradient Desc...\n","                              ...                        \n","6503    “We” vs “I” — How Should You Talk About Yourse...\n","6504                     How Donald Trump Markets Himself\n","6505        Content and Marketing Beyond Mass Consumption\n","6506    5 Questions All Copywriters Should Ask Clients...\n","6507               How To Write a Good Business Blog Post\n","Name: title, Length: 6508, dtype: object"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["medium_data['title']"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["<a name=\"token\"></a>\n","#### Tokenzation\n","\n","Tokenzaion is the process in which we provide an unique id to all the words and make a word index or we can say vocabulary."]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-02-18T11:07:47.534973Z","iopub.status.busy":"2023-02-18T11:07:47.534579Z","iopub.status.idle":"2023-02-18T11:07:47.671542Z","shell.execute_reply":"2023-02-18T11:07:47.670485Z","shell.execute_reply.started":"2023-02-18T11:07:47.534940Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Total number of words:  8238\n","Word: ID\n","------------\n","<oov>:  1\n","Strong:  4\n","And:  8\n","Consumption:  8237\n"]}],"source":["tokenizer = Tokenizer(oov_token='<oov>') # For those words which are not found in word_index\n","tokenizer.fit_on_texts(medium_data['title'])\n","total_words = len(tokenizer.word_index) + 1\n","\n","print(\"Total number of words: \", total_words)\n","print(\"Word: ID\")\n","print(\"------------\")\n","print(\"<oov>: \", tokenizer.word_index['<oov>'])\n","print(\"Strong: \", tokenizer.word_index['strong'])\n","print(\"And: \", tokenizer.word_index['and'])\n","print(\"Consumption: \", tokenizer.word_index['consumption'])"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-02-18T11:07:47.673435Z","iopub.status.busy":"2023-02-18T11:07:47.672913Z","iopub.status.idle":"2023-02-18T11:07:47.996752Z","shell.execute_reply":"2023-02-18T11:07:47.995471Z","shell.execute_reply.started":"2023-02-18T11:07:47.673397Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Total input sequences:  48461\n"]}],"source":["input_sequences = []\n","for line in medium_data['title']:\n","    token_list = tokenizer.texts_to_sequences([line])[0]\n","    #print(token_list)\n","    \n","    for i in range(1, len(token_list)):\n","        n_gram_sequence = token_list[:i+1]\n","        input_sequences.append(n_gram_sequence)\n","\n","# print(input_sequences)\n","print(\"Total input sequences: \", len(input_sequences))"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-02-18T11:07:48.057961Z","iopub.status.busy":"2023-02-18T11:07:48.057420Z","iopub.status.idle":"2023-02-18T11:07:48.065004Z","shell.execute_reply":"2023-02-18T11:07:48.063677Z","shell.execute_reply.started":"2023-02-18T11:07:48.057927Z"},"trusted":true},"outputs":[{"data":{"text/plain":["[[5, 676],\n"," [5, 676, 68],\n"," [5, 676, 68, 2],\n"," [5, 676, 68, 2, 452],\n"," [5, 676, 68, 2, 452, 1518],\n"," [5, 676, 68, 2, 452, 1518, 14],\n"," [5, 676, 68, 2, 452, 1518, 14, 2455],\n"," [5, 676, 68, 2, 452, 1518, 14, 2455, 3653],\n"," [5, 676, 68, 2, 452, 1518, 14, 2455, 3653, 99],\n"," [1858, 23]]"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["input_sequences[:10]"]},{"cell_type":"markdown","metadata":{},"source":["<a name=\"pad\"></a>\n","#### Make all titles with same length by using padding\n","\n","The length of every title has to be the same. To make it, we need to find a title that has a maximum length, and based on that length, we have to pad rest of titles."]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-02-18T11:07:48.125806Z","iopub.status.busy":"2023-02-18T11:07:48.125208Z","iopub.status.idle":"2023-02-18T11:07:48.353748Z","shell.execute_reply":"2023-02-18T11:07:48.352419Z","shell.execute_reply.started":"2023-02-18T11:07:48.125767Z"},"trusted":true},"outputs":[{"data":{"text/plain":["array([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   5, 676,\n","        68], dtype=int32)"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["# pad sequences \n","max_sequence_len = max([len(x) for x in input_sequences])\n","input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n","input_sequences[1]"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2023-02-18T11:07:48.475193Z","iopub.status.busy":"2023-02-18T11:07:48.474819Z","iopub.status.idle":"2023-02-18T11:07:48.483456Z","shell.execute_reply":"2023-02-18T11:07:48.482264Z","shell.execute_reply.started":"2023-02-18T11:07:48.475159Z"},"trusted":true},"outputs":[{"data":{"text/plain":["array([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    5,  676,   68],\n","       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    5,  676,   68,    2],\n","       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    5,  676,   68,    2,  452],\n","       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    5,  676,   68,    2,  452, 1518],\n","       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           5,  676,   68,    2,  452, 1518,   14],\n","       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    5,\n","         676,   68,    2,  452, 1518,   14, 2455],\n","       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    5,  676,\n","          68,    2,  452, 1518,   14, 2455, 3653],\n","       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    5,  676,   68,\n","           2,  452, 1518,   14, 2455, 3653,   99],\n","       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0, 1858,   23]], dtype=int32)"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["input_sequences[1:10\n","               ]"]},{"cell_type":"markdown","metadata":{},"source":["<a name=\"xy\"></a>\n","# Prepare features and labels\n","\n","Here, we consider **last element of all sequences as a label**.Then,\n","We need to perform **onehot encoding on labels corresponding to total_words.**"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2023-02-18T11:09:14.983308Z","iopub.status.busy":"2023-02-18T11:09:14.982909Z","iopub.status.idle":"2023-02-18T11:09:15.167233Z","shell.execute_reply":"2023-02-18T11:09:15.166187Z","shell.execute_reply.started":"2023-02-18T11:09:14.983274Z"},"trusted":true},"outputs":[],"source":["# create features and label\n","xs, labels = input_sequences[:,:-1],input_sequences[:,-1]\n","ys = tf.keras.utils.to_categorical(labels, num_classes=total_words)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2023-02-18T11:09:49.497769Z","iopub.status.busy":"2023-02-18T11:09:49.497034Z","iopub.status.idle":"2023-02-18T11:09:49.506150Z","shell.execute_reply":"2023-02-18T11:09:49.505002Z","shell.execute_reply.started":"2023-02-18T11:09:49.497726Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    5  676   68    2  452 1518]\n","14\n","1.0\n","[0. 0. 0. ... 0. 0. 0.]\n"]}],"source":["print(xs[5])\n","print(labels[5])\n","print(ys[5][14])\n","print(ys[5])"]},{"cell_type":"markdown","metadata":{},"source":["<a name=\"train\"></a>\n","# Bi- LSTM Neural Network Model training"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2023-02-18T11:09:51.761898Z","iopub.status.busy":"2023-02-18T11:09:51.761503Z","iopub.status.idle":"2023-02-18T11:09:51.767304Z","shell.execute_reply":"2023-02-18T11:09:51.765815Z","shell.execute_reply.started":"2023-02-18T11:09:51.761864Z"},"trusted":true},"outputs":[],"source":["from tensorflow.keras import Sequential\n","from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-18T11:09:55.532728Z","iopub.status.busy":"2023-02-18T11:09:55.531955Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/50\n","1515/1515 [==============================] - 151s 97ms/step - loss: 7.0850 - accuracy: 0.0948\n","Epoch 2/50\n","1515/1515 [==============================] - 146s 96ms/step - loss: 5.7734 - accuracy: 0.1725\n","Epoch 3/50\n","1515/1515 [==============================] - 147s 97ms/step - loss: 4.8687 - accuracy: 0.2096\n","Epoch 4/50\n","1515/1515 [==============================] - 146s 96ms/step - loss: 4.0815 - accuracy: 0.2600\n","Epoch 5/50\n","1515/1515 [==============================] - 145s 96ms/step - loss: 3.4870 - accuracy: 0.3228\n","Epoch 6/50\n","1515/1515 [==============================] - 145s 96ms/step - loss: 3.1242 - accuracy: 0.3630\n","Epoch 7/50\n","1515/1515 [==============================] - 146s 96ms/step - loss: 2.9031 - accuracy: 0.3904\n","Epoch 8/50\n","1515/1515 [==============================] - 145s 96ms/step - loss: 2.7152 - accuracy: 0.4164\n","Epoch 9/50\n","1515/1515 [==============================] - 146s 97ms/step - loss: 2.6097 - accuracy: 0.4326\n","Epoch 10/50\n","1515/1515 [==============================] - 145s 95ms/step - loss: 2.4765 - accuracy: 0.4520\n","Epoch 11/50\n","1515/1515 [==============================] - 145s 96ms/step - loss: 2.4035 - accuracy: 0.4617\n","Epoch 12/50\n","1515/1515 [==============================] - 145s 95ms/step - loss: 2.3805 - accuracy: 0.4663\n","Epoch 13/50\n","1515/1515 [==============================] - 146s 97ms/step - loss: 2.2722 - accuracy: 0.4831\n","Epoch 14/50\n","1328/1515 [=========================>....] - ETA: 17s - loss: 2.2519 - accuracy: 0.4828"]}],"source":["model = Sequential()\n","model.add(Embedding(total_words, 100, input_length=max_sequence_len-1))\n","model.add(Bidirectional(LSTM(150)))\n","model.add(Dense(total_words, activation='softmax'))\n","adam = Adam(lr=0.01)\n","model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n","history =  model.fit(xs, ys, epochs=50, verbose=1)\n","#print model.summary()\n","print(model)\n"]},{"cell_type":"markdown","metadata":{},"source":["<a name=\"acc\"></a>\n","# Plotting model accuracy and loss"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-08-15T06:22:51.092945Z","iopub.status.busy":"2021-08-15T06:22:51.092692Z","iopub.status.idle":"2021-08-15T06:22:51.097521Z","shell.execute_reply":"2021-08-15T06:22:51.096585Z","shell.execute_reply.started":"2021-08-15T06:22:51.092921Z"},"trusted":true},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","\n","def plot_graphs(history, string):\n","    plt.plot(history.history[string])\n","    plt.xlabel(\"Epochs\")\n","    plt.ylabel(string)\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-08-15T06:22:51.099252Z","iopub.status.busy":"2021-08-15T06:22:51.098715Z","iopub.status.idle":"2021-08-15T06:22:51.25641Z","shell.execute_reply":"2021-08-15T06:22:51.255568Z","shell.execute_reply.started":"2021-08-15T06:22:51.099215Z"},"trusted":true},"outputs":[],"source":["plot_graphs(history, 'accuracy')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-08-15T06:22:51.257867Z","iopub.status.busy":"2021-08-15T06:22:51.257557Z","iopub.status.idle":"2021-08-15T06:22:51.379243Z","shell.execute_reply":"2021-08-15T06:22:51.378467Z","shell.execute_reply.started":"2021-08-15T06:22:51.257833Z"},"trusted":true},"outputs":[],"source":["plot_graphs(history, 'loss')"]},{"cell_type":"markdown","metadata":{},"source":["<a name=\"new\"></a>\n","# Predicting next word of title"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-08-15T06:22:51.380838Z","iopub.status.busy":"2021-08-15T06:22:51.380501Z","iopub.status.idle":"2021-08-15T06:22:52.015269Z","shell.execute_reply":"2021-08-15T06:22:52.014289Z","shell.execute_reply.started":"2021-08-15T06:22:51.380802Z"},"trusted":true},"outputs":[],"source":["seed_text = \"implementation of\"\n","next_words = 2\n","  \n","for _ in range(next_words):\n","    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n","    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n","    predicted = model.predict_classes(token_list, verbose=0)\n","    output_word = \"\"\n","    for word, index in tokenizer.word_index.items():\n","        if index == predicted:\n","            output_word = word\n","            break\n","    seed_text += \" \" + output_word\n","print(seed_text)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.10"}},"nbformat":4,"nbformat_minor":4}
